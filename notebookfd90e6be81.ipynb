{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12459214,"sourceType":"datasetVersion","datasetId":7859541}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# Load raw input and label text files\nwith open('/kaggle/input/t5model/input_texts.txt', 'r') as f:\n    raw_inputs = f.readlines()\n\nwith open('/kaggle/input/t5model/label_texts.txt', 'r') as f:\n    raw_labels = f.readlines()\n\n# Step 1: Remove custom [sos] and [eos] tokens\ncleaned_inputs = [line.strip().replace('[sos]', '').replace('[eos]', '') for line in raw_inputs]\ncleaned_labels = [line.strip().replace('[sos]', '').replace('[eos]', '') for line in raw_labels]\n\n# Step 2: Add \"chat:\" prefix to inputs only\nfinal_inputs = [f\"chat: {line.strip()}\" for line in cleaned_inputs]\nfinal_labels = [line.strip() for line in cleaned_labels]  # Don't put 'chat:' in labels\n\n# Step 3: Train-test split\nfrom sklearn.model_selection import train_test_split\ntrain_inputs, val_inputs, train_targets, val_targets = train_test_split(\n    final_inputs, final_labels, test_size=0.2, random_state=42\n)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T10:18:17.120184Z","iopub.execute_input":"2025-07-14T10:18:17.120442Z","iopub.status.idle":"2025-07-14T10:18:17.771384Z","shell.execute_reply.started":"2025-07-14T10:18:17.120421Z","shell.execute_reply":"2025-07-14T10:18:17.770615Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install transformers -q\n\nimport os\nimport torch\nimport pandas as pd\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom torch.optim import AdamW   # ‚úÖ NEW\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Load input and target texts\nwith open('/kaggle/input/t5model/input_texts.txt', 'r') as f:\n    input_lines = f.readlines()\n\nwith open('/kaggle/input/t5model/label_texts.txt', 'r') as f:\n    label_lines = f.readlines()\n\nprint(\"Sample Inputs:\", input_lines[:3])\nprint(\"Sample Labels:\", label_lines[:3])\n\n# Split data into training and testing\ntrain_inputs, val_inputs, train_targets, val_targets = train_test_split(\n    input_lines, label_lines, test_size=0.2, random_state=42\n)\n\n# Initialize tokenizer and model\nmodel_name = \"t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Tokenization function\ndef encode_data(sources, targets, tokenizer, max_len=128):\n    source_enc = tokenizer(\n        sources, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\"\n    )\n    target_enc = tokenizer(\n        targets, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\"\n    )\n    return source_enc, target_enc\n\n# Encode training and validation data\ntrain_encodings, train_labels = encode_data(train_inputs, train_targets, tokenizer)\nval_encodings, val_labels = encode_data(val_inputs, val_targets, tokenizer)\n\n# Custom dataset class\nclass ChatbotDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return self.encodings[\"input_ids\"].size(0)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.encodings[\"input_ids\"][idx],\n            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n            \"labels\": self.labels[\"input_ids\"][idx]\n        }\n\n# Create datasets and dataloaders\ntrain_dataset = ChatbotDataset(train_encodings, train_labels)\nval_dataset = ChatbotDataset(val_encodings, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\n\n# Optimizer setup\noptimizer = AdamW(model.parameters(), lr=5e-6)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Training loop\nprint(\"\\nüß† Starting training...\\n\")\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        input_ids = batch[\"input_ids\"].to(device)\n        attn_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{num_epochs} ‚Äî üî• Avg Training Loss: {avg_loss:.4f}\")\n\n# Evaluation loop (first batch)\nprint(\"\\nüîç Evaluating on sample test batch...\\n\")\nmodel.eval()\n\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attn_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        preds = model.generate(input_ids=input_ids, attention_mask=attn_mask, max_length=50)\n\n        inputs_decoded = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n        labels_decoded = [\n            tokenizer.decode(label[label != -100], skip_special_tokens=True)\n            for label in labels\n        ]\n        preds_decoded = [tokenizer.decode(p, skip_special_tokens=True) for p in preds]\n\n        for inp, tgt, pred in zip(inputs_decoded, labels_decoded, preds_decoded):\n            print(\"üó®Ô∏è  User    :\", inp.strip())\n            print(\"‚úÖ Expected:\", tgt.strip())\n            print(\"ü§ñ Predicted:\", pred.strip())\n            print(\"-\" * 60)\n        break  # Only evaluate one batch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T10:18:17.772722Z","iopub.execute_input":"2025-07-14T10:18:17.773229Z","iopub.status.idle":"2025-07-14T10:23:31.452832Z","shell.execute_reply.started":"2025-07-14T10:18:17.773185Z","shell.execute_reply":"2025-07-14T10:23:31.452018Z"}},"outputs":[{"name":"stderr","text":"2025-07-14 10:18:34.593370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752488314.774567      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752488314.825890      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Sample Inputs: ['[sos] hi, how are you doing? [eos]\\n', \"[sos] i'm fine. how about yourself? [eos]\\n\", \"[sos] i'm pretty good. thanks for asking. [eos]\\n\"]\nSample Labels: [\"[sos] i'm fine. how about yourself? [eos]\\n\", \"[sos] i'm pretty good. thanks for asking. [eos]\\n\", '[sos] no problem. so how have you been? [eos]\\n']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82e3d39bab748c9bbbf420f51d564aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97314d49fb9c45ecbdcb6c99f9ae9a87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e17453b03ff4d2486c6d92c4234e26e"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e8c199a5a64ca1970cef9a13949e14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9384390e48f442269de84d83f5321fe5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccc22fce641748208de11f08491d5f22"}},"metadata":{}},{"name":"stdout","text":"\nüß† Starting training...\n\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 ‚Äî üî• Avg Training Loss: 4.6526\nEpoch 2/10 ‚Äî üî• Avg Training Loss: 1.7010\nEpoch 3/10 ‚Äî üî• Avg Training Loss: 1.3167\nEpoch 4/10 ‚Äî üî• Avg Training Loss: 1.0876\nEpoch 5/10 ‚Äî üî• Avg Training Loss: 0.9752\nEpoch 6/10 ‚Äî üî• Avg Training Loss: 0.9262\nEpoch 7/10 ‚Äî üî• Avg Training Loss: 0.8960\nEpoch 8/10 ‚Äî üî• Avg Training Loss: 0.8725\nEpoch 9/10 ‚Äî üî• Avg Training Loss: 0.8576\nEpoch 10/10 ‚Äî üî• Avg Training Loss: 0.8420\n\nüîç Evaluating on sample test batch...\n\nüó®Ô∏è  User    : [sos] would you like to see a movie with me and my friend? [eos]\n‚úÖ Expected: [sos] do you know what movie you're going to watch? [eos]\nü§ñ Predicted: [sos] i'm not sure. [eos]\n------------------------------------------------------------\nüó®Ô∏è  User    : [sos] and we can afford it! [eos]\n‚úÖ Expected: [sos] so are we going to buy it? [eos]\nü§ñ Predicted: [sos] we can afford it! [eos]\n------------------------------------------------------------\nüó®Ô∏è  User    : [sos] i'm not being nosey. i'm just asking. [eos]\n‚úÖ Expected: [sos] i really don't think it's any of your business. [eos]\nü§ñ Predicted: [sos] i'm not being nosey. i'm just asking. [eos]\n------------------------------------------------------------\nüó®Ô∏è  User    : [sos] i will. [eos]\n‚úÖ Expected: [sos] oh, look. here's another shirt just like it. [eos]\nü§ñ Predicted: [sos] i'll be able to get a lot of money. [eos]\n------------------------------------------------------------\nüó®Ô∏è  User    : [sos] it was tomato soup. [eos]\n‚úÖ Expected: [sos] that tasted so good. [eos]\nü§ñ Predicted: [sos] i was a tomato soup. [eos]\n------------------------------------------------------------\nüó®Ô∏è  User    : [sos] yes, he pushed him off the hood of the car. [eos]\n‚úÖ Expected: [sos] no, he said he gently placed the boy on the street. [eos]\nü§ñ Predicted: [sos] he pushed him off the hood of the car. [eos]\n------------------------------------------------------------\nüó®Ô∏è  User    : [sos] i can't believe he won the election. [eos]\n‚úÖ Expected: [sos] only 15 percent of the voters turned out. [eos]\nü§ñ Predicted: [sos] i'm not sure he won the election. [eos]\n------------------------------------------------------------\nüó®Ô∏è  User    : [sos] it hurt her feelings. [eos]\n‚úÖ Expected: [sos] oh. [eos]\nü§ñ Predicted: [sos] she was hurt. [eos]\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"for i in range(5):\n    print(f\"> INPUT:  {train_inputs[i]}\")\n    print(f\"> TARGET: {train_targets[i]}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T10:23:31.453790Z","iopub.execute_input":"2025-07-14T10:23:31.454463Z","iopub.status.idle":"2025-07-14T10:23:31.458820Z","shell.execute_reply.started":"2025-07-14T10:23:31.454430Z","shell.execute_reply":"2025-07-14T10:23:31.458007Z"}},"outputs":[{"name":"stdout","text":"> INPUT:  [sos] it's supposed to start at about eight. [eos]\n\n> TARGET: [sos] how many invitations has she given out? [eos]\n\n\n> INPUT:  [sos] i don't know. [eos]\n\n> TARGET: [sos] when did you lose it? [eos]\n\n\n> INPUT:  [sos] what's going on? [eos]\n\n> TARGET: [sos] nothing really, you? [eos]\n\n\n> INPUT:  [sos] did you hear the news? [eos]\n\n> TARGET: [sos] what happened? [eos]\n\n\n> INPUT:  [sos] i was crossing the street. [eos]\n\n> TARGET: [sos] were you in a crosswalk? [eos]\n\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def chat(input_text, max_length=50, do_sample=False):\n    # Match training format exactly\n    prompt = f\"[sos] {input_text.strip()} [eos]\"\n    encoded = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=128\n    ).to(device)\n\n    output_ids = model.generate(\n        input_ids=encoded[\"input_ids\"],\n        attention_mask=encoded[\"attention_mask\"],\n        max_length=max_length,\n        num_beams=4,\n        do_sample=do_sample,\n        no_repeat_ngram_size=2,\n        early_stopping=True\n    )\n\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return reply\nprint(\"ü§ñ Chatbot is ready! Type 'exit' to quit.\")\nwhile True:\n    user_input = input(\"üßë You: \")\n    if user_input.lower() in [\"exit\", \"quit\"]:\n        print(\"üëã Goodbye!\")\n        break\n    response = chat(user_input)\n    print(f\"ü§ñ Bot: {response}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T10:27:36.778611Z","iopub.execute_input":"2025-07-14T10:27:36.778897Z"}},"outputs":[{"name":"stdout","text":"ü§ñ Chatbot is ready! Type 'exit' to quit.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"üßë You:  What's going on?\n"},{"name":"stdout","text":"ü§ñ Bot: [sos] i don't know what's going on in the future. [eoses]\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"üßë You:  Did you hear the news\n"},{"name":"stdout","text":"ü§ñ Bot: [sos] i'm going to be able to read the news. [eoses]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}